#!/usr/bin/env python3

import json
import torch
import streamlit as st
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation.utils import GenerationConfig
from loguru import logger
from api.baichuan_api import get_symptoms,do_request
from tencent_vector import searchByText
from kg.query import QueryData
import re
from find_diseases import findPossibleDisease

from modeling import load_model_tokenizer, generate  # reletive import

st.set_page_config(page_title="Baichuan 2")
st.title("Baichuan 2")

init_model = st.cache_resource(load_model_tokenizer)


def clear_chat_history():
    del st.session_state.messages


def init_chat_history():
    with st.chat_message("assistant", avatar='ğŸ¤–'):
        st.markdown("æ‚¨å¥½ï¼Œæˆ‘æ˜¯å¿ƒè¡€ç®¡ç–¾ç—…é¢„è¯Šå¤§æ¨¡å‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ğŸ¥°")

    if "messages" in st.session_state:
        for message in st.session_state.messages:
            avatar = 'ğŸ§‘â€ğŸ’»' if message["role"] == "user" else 'ğŸ¤–'
            with st.chat_message(message["role"], avatar=avatar):
                st.markdown(message["content"])
    else:
        st.session_state.messages = []

    return st.session_state.messages


@logger.catch(reraise=True)
def main():
    model, tokenizer = init_model(model_size="13b")    
    qd = QueryData()
    messages = init_chat_history()

    if prompt := st.chat_input("Shift + Enter æ¢è¡Œ, Enter å‘é€"):
        with st.chat_message("user", avatar='ğŸ§‘â€ğŸ’»'):
            st.markdown(prompt)
        messages.append({"role": "user", "content": prompt})
        print(f"[user] {prompt}", flush=True)
        with st.chat_message("assistant", avatar='ğŸ‘©ğŸ»â€âš•ï¸'):
            placeholder = st.empty()

            # ä»promptä¸­æå–éœ€è¦æ£€ç´¢çš„ç—‡çŠ¶
            ymptoms = get_symptoms(prompt) # 
            if ymptoms!='æ— ':
                symptoms_list = re.split(r",|ï¼Œ| |ã€", ymptoms)
                v_symptoms_sim = []
                symptoms_kg_dic = {}
                if len(messages)<=1:
                    for ymptom in symptoms_list:
                        # ä»å‘é‡åº“ä¸­ï¼Œæ£€ç´¢å‡ºç›¸ä¼¼çš„æ ‡å‡†ç—‡çŠ¶è¯
                        v_ymptoms = searchByText(ymptom)
                        v_symptoms_sim = list(set(v_symptoms_sim + v_ymptoms))
                    response = "æ ¹æ®åé¦ˆçš„ç—‡çŠ¶ï¼Œæ‚¨éœ€è¿›ä¸€æ­¥å‡†ç¡®é€‰æ‹©ï¼Œä»¥ä¸‹æ‚¨å¯èƒ½æœ‰çš„ç—‡çŠ¶ï¼š" + 'ã€'.join(v_symptoms_sim)
                    prompt_desc = "è¯·å¯¹ä»¥ä¸‹ç—‡çŠ¶è¿›è¡Œç®€æ´çš„è¯´æ˜ï¼š" + 'ã€'.join(v_symptoms_sim)
                    res_sim = do_request(prompt_desc)

                    response = response + 'ã€‚\n' + res_sim 
                else:
                # ä»çŸ¥è¯†å›¾è°±ä¸­æ£€ç´¢å‡ºç—‡çŠ¶å¯¹åº”çš„ï¼Œå¯åé—®ç”¨æˆ·çš„ç—‡çŠ¶ ä»¥åŠ è¦åŒºåˆ†çš„ç–¾ç—…ã€‚QuerySymptomAndDiseases
                    for symptom in symptoms_list:
                        kg_res = qd.QuerySymptomAndDisease(symptom)
                        for disease in kg_res:
                            symptoms_kg_dic[disease] = kg_res[disease]
                    #symptoms_kg_dic = qd.QuerySymptomAndDiseases(symptoms_list);
                    diseases_to_decide,symptoms_for_ask = findPossibleDisease(symptoms_kg_dic, symptoms_list)
                    if len(symptoms_for_ask)==0 or len(diseases_to_decide) <= 3:
                        response ="ç–‘ä¼¼ç–¾ç—…ä¸ªæ•° " + str(len(diseases_to_decide))  + '  ç–¾ç—…åç§°ï¼š'+ 'ã€'.join(diseases_to_decide)
                        prompt_out = "è¯·ä»ç—…å› ï¼Œç—‡çŠ¶ï¼Œæ£€æŸ¥é¡¹ï¼Œæ²»ç–—æ–¹å¼å››æ–¹é¢å¯¹ä»¥ä¸‹ç–¾ç—…ï¼š" + 'ã€'.join(diseases_to_decide) + "è¿›è¡Œè¯¦ç»†çš„ä»‹ç»"
                        res_out = do_request(prompt_out)
                        response = response  + '\n' + res_out ; 
                    else:
                        response = "æ ¹æ®åé¦ˆçš„ç—‡çŠ¶ï¼Œå¯¹ç–¾ç—… ã€ " + 'ã€'.join(diseases_to_decide) +"ã€‘è¿›ä¸€æ­¥æ’æŸ¥ ï¼Œ \n   è¯·å‘ŠçŸ¥æ‚¨æ˜¯å¦æœ‰ç—‡çŠ¶ï¼š" + 'ã€'.join(symptoms_for_ask)

            else: 
                model = AutoModelForCausalLM.from_pretrained("output1", device_map="auto", torch_dtype=torch.bfloat16, trust_remote_code=True)
                response = generate(prompt, model, tokenizer)
                print(response)
            placeholder.markdown(response)
            if torch.backends.mps.is_available():
                torch.mps.empty_cache()
        messages.append({"role": "assistant", "content": response})
        print(json.dumps(messages, ensure_ascii=False), flush=True)

        st.button("æ¸…ç©ºå¯¹è¯", on_click=clear_chat_history)


if __name__ == "__main__":
    main()
